# SINA-BERT: a pre-trained language model for analysis of medical texts in Persian

SINA-BERT is the first high quality pre-trained language model for Persian medical domain. SINA-BERT utilizes pre-training on a large-scale corpus of medical contents including formal and informal texts collected from a variety of online resources in order to improve the performance on health-care related tasks.

# Model
SINA-BERT has been published in HuggingFace website. You can download it via [this link](https://huggingface.co/).

# Corpora
To obtain corpora used for fine-tuning of SINA-BERT, please don't hesitate us via `nsr.taghizadeh@ut.ac.ir`. Due to the huge size of corpora, we will publish it on demand.


# Evaluation Data
This repository contains datasets of several tasks in area of Persian medical domain. For each task we have prepared a labeled dataset, fine-tuned SINA-BERT and reported the results in our paper.

- Fill-in-the-blank

- Medical Question Classification

- Medical Sentiment Analyze

- Medical Question Retrieval

# References
Please cite following article if using SINA-BERT, or data in this repository.

```bibtex
@article{taghizadeh2021sina,
  title={SINA-BERT: a pre-trained language model for analysis of medical texts in Persian},
  author={Taghizadeh, Nasrin and Doostmohammadi, Ehsan and Seifossadat, Elham and Rabiee, Hamid R and Tahaei, Maedeh S},
  journal={arXiv preprint arXiv:2104.07613},
  year={2021}
}
```

# License
SINA-BERT is GPL-3.0-licensed.
